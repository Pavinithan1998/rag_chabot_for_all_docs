import os
from dotenv import load_dotenv
import streamlit as st
from streamlit_option_menu import option_menu
from streamlit_lottie import st_lottie
from langchain.memory import ConversationBufferMemory
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from markup import doc_qa_tools_demo
from pinecone_actions import ingest_documents
from llm_actions import get_chat_chain
from process_docs import (
    process_pdf,
    process_docx,
    process_txt,
    process_ppt
)
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_core.messages import HumanMessage, AIMessage


load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Define session state key
def initialize_session():
    if "memory" not in st.session_state:
        st.session_state["memory"] = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    if "streamlitchatmehistory" not in st.session_state:
        st.session_state["streamlitchatmehistory"] = StreamlitChatMessageHistory(key="chat_messages")

def tab1():
    st.header(("DocuBot: A document QA Bot"))
    col1, col2 = st.columns([1, 2])
    with col1:
        st_lottie("https://lottie.host/28845468-6375-4bbb-a5b5-40f3742acfd1/puY00pXHQP.json")
       # st.image("image.jpg", use_column_width=True)
    with col2:
        st.markdown(doc_qa_tools_demo(), unsafe_allow_html=True)

def tab2():
    st.header("Manage Vectorstore")
    st.subheader("Upload File")
    uploaded_files = st.file_uploader(
        "Select files", type=["pdf", "docx", "txt", "ppt"], accept_multiple_files=True, key="upload"
    )
    docs = []
    if uploaded_files:
        for file in uploaded_files:
            file_extension = file.name.split(".")[-1].lower()
            file_bytes = file.read()
            if file_extension == "pdf":
                processed_doc = process_pdf(file_bytes, file.name)
                # st.write(processed_doc)
            elif file_extension == "docx":
                processed_doc = process_docx(file_bytes, file.name)
            elif file_extension == "txt":
                processed_doc = process_txt(file_bytes, file.name)
            elif file_extension == "ppt":
                processed_doc = process_ppt(file_bytes, file.name)
            # docs.append(processed_doc)
        st.success("Files processed successfully!")
        status = ingest_documents()
        if status:
            st.success("Files uploaded successfully!")
    else:
        st.info("Upload files to get started.")

from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler

class StreamlitCallbackHandler(BaseCallbackHandler):
    """Custom callback handler to stream responses in Streamlit."""
    def __init__(self, response_placeholder):
        self.response_placeholder = response_placeholder
        self.generated_text = ""

    def on_llm_new_token(self, token: str, **kwargs):
        """Called when a new token is generated by the model."""
        self.generated_text += token
        self.response_placeholder.markdown(self.generated_text)

def tab3(): 
    initialize_session()
    st.header("üó£Ô∏è Chat with the AI about the ingested documents! üìö")
    st.markdown(
        "<p style='font-size: 18px; color: #2a9d8f;'>üí¨ Engage in a dynamic conversation with the AI to explore the ingested documents and gain insights! ü§ñ</p>",
        unsafe_allow_html=True,
    )

    chat_messages = st.session_state["memory"].chat_memory.messages
    for message in chat_messages:
        if isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(
                    f'<p style="font-size: 16px; padding: 10px; border-radius: 10px;">{message.content}</p>',
                    unsafe_allow_html=True,
                )
        elif isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(
                    f'<p style="font-size: 16px;">{message.content}</p></div>',
                    unsafe_allow_html=True,
                )

    if user_input := st.chat_input("Enter your message"):
        with st.chat_message("user"):
            st.markdown(
                f'<p style="font-size: 16px; padding: 10px; border-radius: 10px;">{user_input}</p>',
                unsafe_allow_html=True,
            )

        with st.spinner("Generating Response..."):
            with st.chat_message("assistant"):
                response_placeholder = st.empty()
                streamlit_handler = StreamlitCallbackHandler(response_placeholder)
                stdout_handler = StreamingStdOutCallbackHandler()

                chain = get_chat_chain(memory=st.session_state["memory"])
                ai_response = chain({"question": user_input}, callbacks=[stdout_handler, streamlit_handler])

                ai_message_content = ai_response["answer"]
                response_placeholder.markdown(
                    f'<p style="font-size: 16px;">{ai_message_content}</p></div>',
                    unsafe_allow_html=True,
                )

def main():
    st.set_page_config(page_title="DocuBot", page_icon="üìö", layout="wide")
    
    with st.sidebar:
        app_mode = option_menu(
            menu_title="Choose a page",  
            options=["Home", "Upload & Manage", "Chat with DocuBot"],  
            icons=["house", "upload", "chat"],  
            menu_icon="cast",  
            default_index=0,  
            orientation="vertical"  
        )
    
    # Page selection logic
    if app_mode == "Home":
        tab1()
    elif app_mode == "Upload & Manage":
        tab2()
    elif app_mode == "Chat with DocuBot":
        tab3()

if __name__ == "__main__":
    main()
